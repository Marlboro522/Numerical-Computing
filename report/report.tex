\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, graphicx}

\title{CS 4600/5600 Numerical Computing\\ Homework 1}
\author{Raja}
\date{Fall 2024}

\begin{document}

\maketitle

\section*{Problem 2: Roots - Open Methods}

\subsection*{(b) Newton-Raphson Method}

Given the function:
\[
f(x) = x^3 - 6x^2 + 11x - 6.1
\]

We need to determine the largest positive root using the Newton-Raphson method, starting with \(x_0 = 3.5\).

The Newton-Raphson iteration formula is:
\[
x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}
\]

First, we find the derivative of \(f(x)\):
\[
f'(x) = 3x^2 - 12x + 11
\]

Starting with \(x_0 = 3.5\):
\[
f(3.5) = (3.5)^3 - 6(3.5)^2 + 11(3.5) - 6.1 = 1.775
\]
\[
f'(3.5) = 3(3.5)^2 - 12(3.5) + 11 = 5.75
\]

First iteration:
\[
x_1 = 3.5 - \frac{1.775}{5.75} \approx 3.1913043
\]

Second iteration:
\[
f(3.1913043)  \approx 0.399408
\]
\[
f'(3.1913043)  \approx 3.2576178
\]
\[
x_2 \approx 3.068698
\]

Third iteration:
\[
f(3.068698) = (3.068698)^3 - 6(3.068698)^2 + 11(3.068698) - 6.1 \approx 0.0518784
\]
\[
f'(3.068698) = 3(3.068698)^2 - 12(3.068698) + 11 \approx 2.426346
\]
\[
x_3 = 3.068698 - \frac{0.518784}{2.426346} \approx 3.0473166
\]

The largest positive root after three iterations is approximately \(x \approx 3.0473166\).

\subsection*{(c) Modified Secant Method}

Using the same function \(f(x) = x^3 - 6x^2 + 11x - 6.1\), we apply the modified secant method with \(x_0 = 3.5\) and \(\delta = 0.01\).

The modified secant method iteration formula is:
\[
x_{i+1} = x_n - \frac{\delta x_i f(x_i)}{f(x_i + \delta x_i) - f(x_i)}
\]

\begin{center}
    \begin{tabular}{||c c c||} 
     \hline
     Iteration & $x_i$ & $x_{i+1}$ \\ [0.5ex] 
     \hline\hline
     0 & 3.5 & 3.1995967\\ 
     \hline
     1 & 3.1995967 & 3.0753234  \\
     \hline
     2 & 3.0753234 & 3.04881822 \\
     \hline
     3 & 3.04881822 & 3.0467729 \\
     \hline
     4 & 3.0467729 & 3.0466842863 \\
     \hline
    \end{tabular}
\end{center}

The largest positive root after five iterations \\
is approximately \(x \approx 3.0466842863\).

\subsection*{Commets on results: }Both methods, are closer to the root. The Nweton-Raphson method showed faster convergence for roots. 
\section*{Problem 3:}
As the iteration count increased. The function value neared to 0. The diffence is the sensitivity to the initial value givenn. 
\section*{Problem 4: Optimization}

Given the function:
\[
f(x) = 4x - 1.8x^2 + 1.2x^3 - 0.3x^4
\]

\subsection*{(b) Prove the function is concave for all values of \(x\)}

A function is concave if its second derivative is negative for all values of \(x\).

First, we find the first derivative of \(f(x)\):
\[
f'(x) = 4 - 3.6x + 3.6x^2 - 1.2x^3
\]

Now, we find the second derivative:
\[
f''(x) = -3.6 + 7.2x - 3.6x^2
\]

For \(f''(x)\) to be negative for all values of \(x\), we need to check its sign:
\[
f''(x) = -3.6 + 7.2x - 3.6x^2
\]

Let's analyze the roots of \(f''(x)\):
\[
-3.6 + 7.2x - 3.6x^2 = 0
\]
\[
3.6x^2 - 7.2x + 3.6 = 0
\]
\[
x^2 - 2x + 1 = 0
\]
\[
(x - 1)^2 = 0
\]
\[
x = 1
\]

At \(x = 1\):
\[
f''(1) = -3.6 + 7.2(1) - 3.6(1)^2 = -3.6 + 7.2 - 3.6 = 0
\]

The second derivative is zero at \(x = 1\), so we need to analyze the behavior around \(x = 1\):
For \(x < 1\), \(f''(x)\) is negative.
For \(x > 1\), \(f''(x)\) is negative.

 \(f''(x)\) doesn't change sign, the function is concave for all values of \(x\).

\subsection*{Problem 5: Optimization}

Given the function:
\[
f(x_1, x_2) = \frac{1}{2}(x_1^2 - x_2)^2 + \frac{1}{2}(1 - x_1)^2
\]

\subsection*{(a) Minimum point}

To find the minimum point, we need to find the Hessian Matrix:
\[
% H = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2} \right) = 0
H = \begin{pmatrix}
6x_1^2 - 2x_2 + 1 & -2x_1 \\
-2x_1 & 1
\end{pmatrix}
\]

Simplifying, we find the critical point is:
\[
(x_1, x_2) = (1, 1)
\]

After substituing the \((x_1, x_2)\) as (1,1) the \(\det{H}\) is greater than 0.

Therefore, the critical point is the minimum point of the function.

\subsection*{(b) Newton's Method Iteration}

Using the starting point \(\begin{pmatrix} 2 \\ 2 \end{pmatrix}\), we perform one iteration of Newton's method for minimizing \(f\).

The Hessian matrix is:
\[
H = \begin{pmatrix}
6x_1^2 - 2x_2 + 1 & -2x_1 \\
-2x_1 & 1
\end{pmatrix}
\]

The gradient matrix would be \(\begin{pmatrix}
    \frac{\partial f} { \partial x_1} \\
    \frac{\partial f}  {\partial x_2}
\end{pmatrix}\)
\\ 
\(
    \frac{\partial f} {\partial x_1} = 2x_1(x_1^2 - x_2) - (1 - x_1) \\
    \frac{\partial f}  {\partial x_2} = -(x_1^2 - x_2)
\)

The gradient at \(\begin{pmatrix} 2 \\ 2 \end{pmatrix}\) is:
\[
\nabla f = \begin{pmatrix}
2x_1(x_1^2 - x_2) - (1 - x_1) \\
-(x_1^2 - x_2)
\end{pmatrix}
= \begin{pmatrix}
2(2)(4 - 2) - (1 - 2) \\
-(4 - 2)
\end{pmatrix}
= \begin{pmatrix}
9 \\
-2
\end{pmatrix}
\]

The Hessian at \(\begin{pmatrix} 2 \\ 2 \end{pmatrix}\) is:
\[
H = \begin{pmatrix}
6(2)^2 - 2(2) + 1 & -2(2) \\
-2(2) & 1
\end{pmatrix}
= \begin{pmatrix}
21 & -4 \\
-4 & 1
\end{pmatrix}
\]

The Newton step is:
\[
\Delta x = -H^{-1} \nabla f
\]

We need to invert the Hessian:

Formula:
\[
H^{-1} = \frac{1}{ad - bc} \begin{pmatrix}
d & -b \\
-c & a
\end{pmatrix}
\]

\[
H^{-1} = \frac{1}{21 \cdot 1 - (-4 \cdot -4)} \begin{pmatrix}
1 & 4 \\
4 & 21
\end{pmatrix}
= \frac{1}{5} \begin{pmatrix}
1 & 4 \\
4 & 21
\end{pmatrix}
= \begin{pmatrix}
0.2 & 0.8 \\
0.8 & 4.2
\end{pmatrix}
\]

The Newton step is:\\
$
\Delta x = - \begin{pmatrix}
0.2 & 0.8 \\
0.8 & 4.2
\end{pmatrix} \begin{pmatrix}
9 \\
-2
\end{pmatrix}\\
= - \begin{pmatrix}
 0.2 \cdot 9 + 0.8 \cdot (-2) \\
 0.8 \cdot 9+ 4.2\cdot (-2)
\end{pmatrix}\\
= - \begin{pmatrix}
1.8 -1.6 \\
7.2 -8.4
\end{pmatrix}\\
= - \begin{pmatrix}
0.2 \\
-1.2
\end{pmatrix}\\
= \begin{pmatrix}
-0.2 \\
1.2
\end{pmatrix}
$

The new point after one iteration is:
\[
\begin{pmatrix}
2 \\
2
\end{pmatrix}
+ \begin{pmatrix}
-0.2 \\
1.2
\end{pmatrix}
= \begin{pmatrix}
1.8\\
2.2
\end{pmatrix}
\]

To determine if this is a good step or a bad one, we need to evaluate the function at this new point,
and see how closer it is the point we found as minima which is \((1,1)\)\\
if \(f(1.8,2.2) < f(2,2)\) then it would be a good step because\\
\(
    f(1.8,2.2) = \frac{1}{2}(1.8^2 - 2.2)^2 + \frac{1}{2}(1 - 1.8)^2 = 0.02 \\
    f(2,2) = \frac{1}{2}(2^2 - 2)^2 + \frac{1}{2}(1 - 2)^2 = 1\\
    f(1,1) = \frac{1}{2}(1^2 - 1)^2 + \frac{1}{2}(1 - 1)^2 = 0
    \)
\paragraph*{Sense of why it is good step: }When we evaluate the function with the given values, we can see that new values are making the function to near 0, which is the minimum value at point (1,1) \\
Therefore, the new point is a good step in this sense.

\paragraph*{Sense of why this could be a bad step: }By lookinng at just the values of of \(x_1\) and \(x_2\) we can see that \(x_2\) value increased \\
There is an 

\section*{Graduate Problem: }
\paragraph*{My understandding:}
The projectile motion is parabolic and should pass thorough the point, \((x1,h2)\) inorder to clear the roof. 
The hint says the coverage is maximized when the water just clears the height of the roof, which is h2.
So, the coordinates to clear the front of the roof are \((x1,h2)\) and the maximum it can go is at the coordinate \((x2,h2)\).
The distance between those two points is just \(x2-x1\). 

The equation of the projectile motion is given by:
\(y = \tan\theta(x) - \frac{gx^2}{2(ucos\theta)^2}\).
I need to think about this analytically. 

So, if we can find the height of the projection, at some x, we can make always make it go..beyond 10m.

\paragraph*{About the Code: }
I have written a code to find the maximum distance the water can go, given the height of the roof and the angle of projection.
The variable tflight is evaluated so that we can get x2 out using the formula \(x_2=x_1 + v \cdot \cos\theta \cdot t_f\)
The maximum distance is calculated as \(x_2 - x_1\).
The method used for Optimization is \textit{Sequenntial Least Square Programmming} which also uses gradients, which leadds to faster convergence. I chose this method because it can support constraints. 

\begin{quote}
    \textbf{"All the code is in the file graph.py}
\end{quote}
\end{document}